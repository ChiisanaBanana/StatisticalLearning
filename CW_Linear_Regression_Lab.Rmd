---
title: "Ch 3: Linear Regression Lab"
author: "Halan Badilla" 
output: html_notebook
---

# Library

Make sure to install the tidyverse package if you don't already have it.
```{r}
library(tidyverse)
```


# Data

Download the _Advertising.csv_ file from canvas in this weeks module and load the Advertising Data. (Make sure to save the csv in the same file that you've saved your .rmd)

```{r}
#Load the advertising data
setwd("C:/Users/halan/Desktop/FPU2025Spring/Statistical Learning")
SalesData <- read.csv("Advertising.csv")

```

The Advertising data set consists of the `sales` of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: `TV, radio, and newspaper`.

# Summary Statistics
```{r}
#Check the first several observations
dim(SalesData)
```

```{r}
head(SalesData)
```

```{r}
# Look at a summary of the data
summary(SalesData)
```

#Answer; I do not have sufficient information to declare if this is accurate or not however I do see no negative values, and I see that TV has highest max followed by newspaper then radio. The inputs here are TV, Radio and Newspaper, the output is sales for our purposes.

Lets take a look at a histogram of our data: 
```{r}
#plot a histogram of each variable

hist(SalesData$TV, col = "pink")


```


# Simple Linear Regression Model

### WHAT:
Linear regression models are a useful tool for predicting a _quantitative_ response.

### WHY:
The goal is to investigate the relationship between Sales and money spent on advertising for either TV, Radio or Newspaper. 

### HOW:
We can create a simple linear model of the form y=β0+β1x+ϵ, where the _response/dependent_ variable y is sales, and the _independent/explanatory_ variable x represents money spent in advertising in TV, radio or newspaper.

```{r}
# Simple Linear Models
lm_TV <- lm(sales ~ TV, data = SalesData)
lm_radio <- lm(sales ~ radio, data = SalesData)
lm_newspaper <- lm(sales ~ newspaper, data = SalesData)
```

Now let's plot the different regression lines obtained from the linear models we just made:

```{r}
attach(SalesData)
```

```{r}
#Plot only the model for Sales vs TV
par(mfrow = c(1, 3)) # Setting graph parameters: 1 row and 3 columns

# Sales
plot(TV, sales)
abline(lm_TV, col = "red", lwd = 4)

# Radio
plot(radio, sales)
abline(lm_radio, col = "red", lwd = 4)

# Newspaper
plot(newspaper, sales)
abline(lm_newspaper, col = "red", lwd = 4)
```

Let us focus on one model Sales vs TV 
```{r}
# Plot only the model for Sales vs TV

plot(TV, sales, pch = 19)
abline(lm_TV, col = "red", lwd = 3)

```

```{r}
#Look at a summary of the Sales v TV linear regression model

summary(lm_TV)
```

## Coefficients:

Theoretically, in simple linear regression, _the coefficients are two unknown constants that represent the intercept, β0, and slope, β1_, terms in the linear model. 
Ultimately, an analyst wants to find an intercept and a slope such that the resulting fitted line is as close as possible to the data points in our data set.

## Significance test:

In the linear regression model y=β0+β1x+ϵ, we can decide whether there is any significant relationship between x and y by testing the null hypothesis that β1=0.

The significance test studies the null hypothesis H0 that claims that there is no relationship between x and y, that is

H0:β1=0

H1:β1≠0

Notice that if β1=0 then y=β0+ϵ.

_In this case_, there is significant evidence against the null hypothesis (p-value < 0.005), so we can conclude that there exists a relation between the number of Sales (y) and the amount of money spent in TV advertising (x).

## Standard  Error:

The coefficient Standard Error _measures the average amount that the coefficient estimates vary from the actual average value of our response variable_.  We would ideally want a lower number relative to its coefficients.

In  our  example,  we  have  previously  determined  that  for  every  one dollar 1  increase  in  the  amount spent on advertising in TV,  the sales go up by 0.04 dollars.  The Standard Error can be used to compute an estimate of the expected difference in case we ran the model again and again.The Standard Errors can also be used to compute confidence intervals and to statistically test the hypothesis of the existence of a relationship between the dependent and independent variables.

## t value:
The coefficient t-value is _a measure of how many standard deviations our coefficient estimate is far away from 0_.  We want it to be far away from zero as this would indicate we could reject the null hypothesis - that is, we could declare that a relationship between dependent and independent variable exists.

In our example, the t-statistic values are relatively far away from zero and are large relative to the standard error, which could indicate a relationship exists.  In general, t-values are also used to compute p-values.


## Pr(>|t|):
The Pr(>|t|) acronym found in the model output _relates to the probability of observing any value equal or larger than t_.  A small p-value indicates that it is unlikely we will observe a relationship between the predictor and response variables due  to  chance.
Typically, a p-value of 5% or less is a good cut-off point.  In our model example, the p-values are very close to zero.  Note the Signif. codes associated to each estimate.  Three stars (or asterisks) represent a highly significant p-value.  
Consequently, a small p-value for the intercept and the slope indicates that we can reject the null hypothesis which allows us to conclude that there is a relationship between the predictor and response variables.

## Residual  Standard  Error:
Residual Standard Error is _a measure of the quality of a linear regression fit_._  Theoretically,  every linear model is assumed to contain an error term ε.  Due to the presence of this error term, we are not capable of perfectly predicting  our response variable  from the predictor.
The  Residual  Standard  Error  is  the  average  amount  that  the  response  will  deviate  from  the  true regression line.  In our example, the sales can deviate from the true regression line by approximately 3.259, on average.


## Multiple  R-squared,  Adjusted  R-squared:
The R-squared (R2) statistic provides _a measure of how well the model is fitting the actual data.  It takes the form of a proportion  of  variance_.  R2 is a measure of the linear relationship between our predictor variable and our response / target variable.  It always lies between 0 and 1 (i.e.:  a number near 0 represents a regression that does not explain the variance in the response variable well and a number close to 1 does explain the observed variance in the response variable).
In our example,  the R2  we get is 0.6119.  Or roughly 61% of the variance found in the response variable can be explained by the predictor variable.
What R-Squared tells us is the proportion of variation in the dependent (response) variable that has been explained by this model.

`A side note`:  In multiple regression settings, the R2 will always increase as more variables are included in the model.  That is why the adjusted R2 is the preferred measure as it adjusts for the number of variables considered.
	

## F-Statistic
F-statistic  is  _a  good  indicator  of  whether  there  is  a  relationship  between  our  predictor  and  the  response variables_.  The further the F-statistic is from 1 the better it is.  However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors.

Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis (H0 :  There is no relationship between sales and TV).
The  reverse  is  true  as  if  the  number  of  data  points  is  small,  a  large  F-statistic  is  required  to  be  able  to ascertain that there may be a relationship between predictor and response variables.  In our example the F-statistic  is relatively larger than 1 given the size of our data.

We can also find the confidence interval for the coefficients of the regression line β0 and β1:

```{r}
# confidence coefficients
confint(lm_TV)
```

# Multiple Linear Regression Model

Here our model is given by:  y=β0+β1x1+β2x2+β3x3+ϵ

We can interpret βj as the average effect on y of a one unit increase in xj, holding all other predictors fixed.

First, look at correlations between the variables:
```{r}
#use the cor() function in R to look at correlation between variables
cor(SalesData)
#use the pairs() function in R to visualize the relationships
pairs(SalesData)
```

Now, let's look at a linear regression model with all of the variables in the data set
```{r}
#Make a multiple linear regression model in R using the same lm() function from before
MLR_Sales <- lm(sales ~ . + TV*radio, SalesData)
# MLR_Sales <- lm(sales ~ TV + radio + newspaper + TV*Radio, SalesData)
```

```{r}
summary(MLR_Sales)
```

Notes for submitting HW assignments: You will be submitting .html files. To do this:
1. Make sure you have hit run on all chunks. 
2. Click the preview button located between the magnifying glass and the cog. (the preview should appear in the viewer window)
4. Click the show in new window tab located next to the broom to preview your submission
5. In the new window, after verifying that everything looks correct, right click and save as: Name and store your .html file appropriately 

